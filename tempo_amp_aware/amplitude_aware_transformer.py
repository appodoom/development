# -*- coding: utf-8 -*-
"""amplitude_aware_transformer_amer.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1aPVoWFkzXB8MoFIiuDAcZrqsjb9Bxovz
"""

import os
import json
import math
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from google.colab import drive

root            = '/content'
save_dir        = os.path.join(root, '')
vocab_path      = os.path.join(root, 'vocab_music_sheet_2500.txt')
tokens_dir      = os.path.join(root, 'tokens_amp')  # contains tokens_sample_{0..13}.json

# --- Build string_to_index / index_to_string ---
with open(vocab_path, encoding='utf-8') as f:
    tokens = [t.strip() for t in f if t.strip()]
string_to_index = {tok: i for i, tok in enumerate(tokens)}
index_to_string = {i: tok for tok, i in string_to_index.items()}
vocab_size = len(string_to_index)
print(f"vocab_size: {vocab_size}")

# --- Preload all 14 JSON token lists into memory once ---
all_data_tensors = []
for i in range(14):
    path = os.path.join(tokens_dir, f'tokens_corpus{i}.json')
    with open(path, encoding='utf-8') as f:
        token_list = json.load(f)
    ids = [string_to_index[t] for t in token_list if t in string_to_index]
    all_data_tensors.append(torch.tensor(ids, dtype=torch.long))
print(f"Loaded {len(all_data_tensors)} token lists into memory")

# --- In-Memory Dataset ---
class InMemoryTextDataset(Dataset):
    def __init__(self, data_tensor, context_size=512):
        self.data = data_tensor
        self.context_size = context_size

    def __len__(self):
        return self.data.size(0) - self.context_size

    def __getitem__(self, idx):
        x = self.data[idx : idx + self.context_size]
        y = self.data[idx + 1 : idx + 1 + self.context_size]
        return x, y

# --- Relative Multi-Head Attention ---
class RelativeMultiheadAttention(nn.Module):
    def __init__(self, embed_dim, num_heads, max_len=512, dropout=0.0):
        super().__init__()
        assert embed_dim % num_heads == 0
        self.embed_dim = embed_dim
        self.num_heads = num_heads
        self.head_dim  = embed_dim // num_heads

        self.q_proj = nn.Linear(embed_dim, embed_dim)
        self.k_proj = nn.Linear(embed_dim, embed_dim)
        self.v_proj = nn.Linear(embed_dim, embed_dim)
        self.out_proj = nn.Linear(embed_dim, embed_dim)
        self.dropout = nn.Dropout(dropout)

        self.max_len  = max_len
        self.rel_bias = nn.Parameter(torch.zeros(num_heads, 2 * max_len - 1))

    def forward(self, x, attn_mask=None):
        B, L, E = x.size()
        H, D    = self.num_heads, self.head_dim

        q = self.q_proj(x).view(B, L, H, D).transpose(1, 2)
        k = self.k_proj(x).view(B, L, H, D).transpose(1, 2)
        v = self.v_proj(x).view(B, L, H, D).transpose(1, 2)

        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(D)

        positions = torch.arange(L, device=x.device)
        rel_idx   = positions.unsqueeze(1) - positions.unsqueeze(0) + (self.max_len - 1)
        bias      = self.rel_bias[:, rel_idx]               # (H, L, L)
        scores    = scores + bias.unsqueeze(0)              # (B, H, L, L)

        if attn_mask is not None:
            scores = scores.masked_fill(attn_mask.unsqueeze(0).unsqueeze(1), float('-inf'))

        attn    = torch.softmax(scores, dim=-1)
        attn    = self.dropout(attn)
        context = torch.matmul(attn, v)                     # (B, H, L, D)

        context = context.transpose(1, 2).contiguous().view(B, L, E)
        return self.out_proj(context)

class RelativeTransformerEncoderLayer(nn.Module):
    def __init__(self, d_model, nhead, dim_feedforward=1024, dropout=0.1, max_len=512):
        super().__init__()
        self.self_attn = RelativeMultiheadAttention(d_model, nhead, max_len=max_len, dropout=dropout)
        self.linear1   = nn.Linear(d_model, dim_feedforward)
        self.linear2   = nn.Linear(dim_feedforward, d_model)
        self.norm1     = nn.LayerNorm(d_model)
        self.norm2     = nn.LayerNorm(d_model)
        self.dropout1  = nn.Dropout(dropout)
        self.dropout2  = nn.Dropout(dropout)

    def forward(self, src, src_mask=None):
        src2 = self.self_attn(src, attn_mask=src_mask)
        src  = src + self.dropout1(src2)
        src  = self.norm1(src)

        ff  = F.relu(self.linear1(src))
        ff2 = self.linear2(self.dropout2(ff))
        src = src + self.dropout2(ff2)
        return self.norm2(src)

# --- MusicTransformer with Relative Positional Encoding ---
class MusicTransformer(nn.Module):
    def __init__(self, vocab_size, d_model=256, nhead=4, num_layers=4,
                 dim_feedforward=1024, max_len=1024):
        super().__init__()
        self.token_emb  = nn.Embedding(vocab_size, d_model)
        layers = []
        for _ in range(num_layers):
            layers.append(RelativeTransformerEncoderLayer(
                d_model=d_model, nhead=nhead,
                dim_feedforward=dim_feedforward,
                dropout=0.1, max_len=max_len
            ))
        self.transformer = nn.Sequential(*layers)
        self.out         = nn.Linear(d_model, vocab_size)

    def forward(self, x):
        emb     = self.token_emb(x)                                    # (B, L, D)
        seq_len = x.size(1)
        mask    = torch.triu(torch.ones(seq_len, seq_len, device=x.device), diagonal=1).bool()
        h = emb
        for layer in self.transformer:
            h = layer(h, src_mask=mask)
        return self.out(h)

# --- Prepare DataLoaders ---
context_size = 1024
batch_size   = 8

train_loaders = [
    DataLoader(
        InMemoryTextDataset(t, context_size=context_size),
        batch_size=batch_size, shuffle=True, drop_last=True
    )
    for t in all_data_tensors
]

# --- Training Setup ---
device    = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model     = MusicTransformer(vocab_size).to(device)
optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)
criterion = nn.CrossEntropyLoss()

epochs = 5
for ep in range(1, epochs + 1):
    model.train()
    total_loss    = 0.0
    total_batches = 0

    for loader in train_loaders:
        for xb, yb in loader:
            xb, yb = xb.to(device), yb.to(device)
            logits = model(xb)  # (B, L, V)
            loss   = criterion(logits.view(-1, vocab_size), yb.view(-1))
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            total_loss    += loss.item()
            total_batches += 1
            if total_batches % 10 == 0:
                print(f"Epoch {ep} batch {total_batches} loss {total_loss/total_batches:.4f}", flush=True)

    avg_loss = total_loss / total_batches
    print(f"Epoch {ep:2d} — loss: {avg_loss:.4f}", flush=True)

    # save checkpoint
    ckpt = {
        'epoch':                ep,
        'model_state_dict':     model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'loss':                 avg_loss,
    }
    torch.save(ckpt, os.path.join(save_dir, f'checkpoint_ep{ep}.pt'))

# final weights only
torch.save(model.state_dict(),
           os.path.join(save_dir, 'music_sheet_transformer_weights.pth'))
print("✅ Training complete! All checkpoints and final weights are in", save_dir)